{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets, utils\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring metrics and CV score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any manual feature engineering before column transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_averages= [\"january_avg_temp\",\"february_avg_temp\",\"march_avg_temp\",\"april_avg_temp\",\"may_avg_temp\",\n",
    "                \"june_avg_temp\",\"july_avg_temp\",\"august_avg_temp\",\"september_avg_temp\",\"october_avg_temp\",\"november_avg_temp\",\n",
    "                \"december_avg_temp\"]\n",
    "temp_mins= [\"january_min_temp\",\"february_min_temp\",\"march_min_temp\",\"april_min_temp\",\"may_min_temp\",\n",
    "                \"june_min_temp\",\"july_min_temp\",\"august_min_temp\",\"september_min_temp\",\"october_min_temp\",\"november_min_temp\",\n",
    "                \"december_min_temp\"]\n",
    "temp_max= [\"january_max_temp\",\"february_max_temp\",\"march_max_temp\",\"april_max_temp\",\"may_max_temp\",\n",
    "                \"june_max_temp\",\"july_max_temp\",\"august_max_temp\",\"september_max_temp\",\"october_max_temp\",\"november_max_temp\",\n",
    "                \"december_max_temp\"]\n",
    "\n",
    "df[\"months_above_65\"] =(df[temp_averages] >=65).sum(axis=1)\n",
    "df[\"months_below_65\"] =(df[temp_averages] <65).sum(axis=1)\n",
    "df[\"months_min_below_65\"] = (df[temp_mins] <65).sum(axis=1)\n",
    "df[\"months_min_above_65\"] = (df[temp_mins] >=65).sum(axis=1)\n",
    "df[\"months_max_below_65\"] = (df[temp_max] <65).sum(axis=1)\n",
    "df[\"months_max_above_65\"] = (df[temp_max] >=65).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_class = pd.read_csv(\"f_type.csv\")\n",
    "df = pd.merge(df, facility_class, on=\"facility_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value = df[\"direction_max_wind_speed\"]\n",
    "# df['dir_max_wind_speed'] = np.where(value > 337.5, \"N\",\n",
    "#                                 np.where(value > 292.5, \"NE\",\n",
    "#                                         np.where(value > 247.5, \"E\",\n",
    "#                                                  np.where(value > 202.5, \"SE\",\n",
    "#                                                           np.where(value > 157.5, \"S\",\n",
    "#                                                                    np.where(value > 112.5, \"SW\",\n",
    "#                                                                             np.where(value > 67.5, \"W\",\n",
    "#                                                                                      np.where(value > 22.5, \"NW\", \"N\"))))))))\n",
    "\n",
    "# value = df[\"direction_peak_wind_speed\"]\n",
    "# df['dir_peak_wind_speed'] = np.where(value > 337.5, \"N\",\n",
    "#                                 np.where(value > 292.5, \"NE\",\n",
    "#                                         np.where(value > 247.5, \"E\",\n",
    "#                                                  np.where(value > 202.5, \"SE\",\n",
    "#                                                           np.where(value > 157.5, \"S\",\n",
    "#                                                                    np.where(value > 112.5, \"SW\",\n",
    "#                                                                             np.where(value > 67.5, \"W\",\n",
    "#                                                                                      np.where(value > 22.5, \"NW\", \"N\"))))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group columns for transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"site_eui\"\n",
    "\n",
    "numeric_features = [\n",
    "    \"floor_area\",\n",
    "    \"year_built\",\n",
    "    \"energy_star_rating\",\n",
    "    \"cooling_degree_days\",\n",
    "    \"heating_degree_days\",\n",
    "    \"precipitation_inches\",\n",
    "    \"snowdepth_inches\",\n",
    "    \"avg_temp\",\n",
    "    \"january_avg_temp\",\n",
    "    \"february_avg_temp\",\n",
    "    \"march_avg_temp\",\n",
    "    \"april_avg_temp\",\n",
    "    \"may_avg_temp\",\n",
    "    \"june_avg_temp\",\n",
    "    \"july_avg_temp\",\n",
    "    \"august_avg_temp\",\n",
    "    \"september_avg_temp\",\n",
    "    \"october_avg_temp\",\n",
    "    \"november_avg_temp\",\n",
    "    \"december_avg_temp\",\n",
    "    \"days_below_30F\",\n",
    "    \"days_above_80F\",\n",
    "    \"max_wind_speed\",\n",
    "    \"months_above_65\",\n",
    "    \"months_below_65\",\n",
    "    \"months_min_below_65\",\n",
    "    \"months_min_above_65\",\n",
    "    \"months_max_below_65\",\n",
    "    \"months_max_above_65\",\n",
    "    \"snowfall_inches\",\n",
    "    \"january_min_temp\",\n",
    "    \"january_max_temp\",\n",
    "    \"february_max_temp\",\n",
    "    \"february_min_temp\",\n",
    "    \"march_min_temp\",\n",
    "    \"march_max_temp\",\n",
    "    \"april_min_temp\",\n",
    "    \"april_max_temp\",\n",
    "    \"may_min_temp\",\n",
    "    \"may_max_temp\",\n",
    "    \"june_min_temp\",\n",
    "    \"june_max_temp\",\n",
    "    \"july_min_temp\",\n",
    "    \"july_max_temp\",\n",
    "    \"august_min_temp\",\n",
    "    \"august_max_temp\",\n",
    "    \"september_min_temp\",\n",
    "    \"september_max_temp\",\n",
    "    \"october_min_temp\",\n",
    "    \"october_max_temp\",\n",
    "    \"november_min_temp\",\n",
    "    \"november_max_temp\",\n",
    "    \"december_min_temp\",\n",
    "    \"december_max_temp\",\n",
    "    \"ELEVATION\",\n",
    "    \"days_below_20F\",\n",
    "    \"days_below_10F\",\n",
    "    \"days_below_0F\",\n",
    "    \"days_above_90F\",\n",
    "    \"days_above_100F\",\n",
    "    \"days_above_110F\"\n",
    "]\n",
    "\n",
    "ordinal_features = []\n",
    "categorical_features = [\"State_Factor\",\n",
    "                        \"building_class\",\n",
    "                        'facility_class']\n",
    "\n",
    "drop_features = [\n",
    "    \"id\",\n",
    "    \"days_with_fog\",\n",
    "    \"facility_type\",\n",
    "    \"direction_max_wind_speed\",\n",
    "    \"direction_peak_wind_speed\",\n",
    "    \"Year_Factor\",\n",
    "]\n",
    "\n",
    "assert df.columns.shape[0] == len(\n",
    "    numeric_features\n",
    "    + ordinal_features\n",
    "    + categorical_features\n",
    "    + [target]\n",
    "    + drop_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=123)\n",
    "X_train, y_train = train_df.drop(columns=[target]), train_df[target]\n",
    "X_test, y_test = test_df.drop(columns=[target]), test_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column transformation & preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"constant\", fill_value=0), StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "column_names = (\n",
    "    numeric_features\n",
    "    + preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(\n",
    "    X_train_transformed, columns=column_names, index=X_train.index\n",
    ")\n",
    "\n",
    "X_valid_transformed = preprocessor.fit_transform(X_test)\n",
    "\n",
    "X_valid_transformed_df = pd.DataFrame(\n",
    "    X_valid_transformed, columns=column_names, index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class energy_model(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 150),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            torch.nn.Linear(150, 50),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            torch.nn.Linear(50, 30),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            torch.nn.Linear(30, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    \n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        \n",
    "        train_batch_loss = 0\n",
    "        train_batch_acc = 0\n",
    "        valid_batch_loss = 0\n",
    "        valid_batch_acc = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            y_hat = model(X).flatten()\n",
    "            loss = criterion(y_hat, y)   # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "            train_batch_acc += (torch.sqrt(torch.mean((y_hat-y)**2))*-1).type(torch.float32).item()   \n",
    "            \n",
    "        train_loss.append(train_batch_loss / len(trainloader))     # loss = total loss in epoch / number of batches = loss per batch\n",
    "        train_accuracy.append(train_batch_acc / len(trainloader))  # accuracy\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()  # this turns off those random dropout layers, we don't want them for validation!\n",
    "        \n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "            for X, y in validloader:\n",
    "                y_hat = model(X).flatten()  # Forward pass to get output\n",
    "                loss = criterion(y_hat, y)   # Calculate loss based on output\n",
    "                valid_batch_loss += loss.item()                  # Add loss for this batch to running total\n",
    "                valid_batch_acc += (torch.sqrt(torch.mean((y_hat-y)**2))*-1).type(torch.float32).item()   # Average accuracy for this batch\n",
    "                \n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n",
    "        \n",
    "        model.train()  # turn back on the dropout layers for the next training loop\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1:3}:\",\n",
    "                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n",
    "                  f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n",
    "                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 0 and valid_loss[-1] > valid_loss[-2]:\n",
    "            consec_increases += 1\n",
    "        else:\n",
    "            consec_increases = 0\n",
    "        if consec_increases == patience:\n",
    "            print(f\"Stopped early at epoch {epoch + 1:3}: val loss increased for {consec_increases} consecutive epochs!\")\n",
    "            break\n",
    "    \n",
    "    results = {\"train_loss\": train_loss,\n",
    "               \"valid_loss\": valid_loss,\n",
    "               \"train_accuracy\": train_accuracy,\n",
    "               \"valid_accuracy\": valid_accuracy}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = np.shape(X_train_transformed_df)[1]\n",
    "model = energy_model(input_size)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =0.001)\n",
    "\n",
    "\n",
    "train_target = torch.tensor(y_train.values.astype(np.float32))\n",
    "train = torch.tensor(X_train_transformed_df.values.astype(np.float32)) \n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor,  batch_size=15, shuffle=True)\n",
    "\n",
    "\n",
    "valid_target = torch.tensor(y_test.values.astype(np.float32))\n",
    "valid = torch.tensor(X_valid_transformed_df.values.astype(np.float32)) \n",
    "valid_tensor = data_utils.TensorDataset(valid, valid_target) \n",
    "valid_loader = data_utils.DataLoader(dataset = valid_tensor,  batch_size=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss: 31.879. Valid Loss: 25.715. Train Accuracy: -48.45. Valid Accuracy: -40.95.\n",
      "Epoch   2: Train Loss: 29.250. Valid Loss: 25.493. Train Accuracy: -45.60. Valid Accuracy: -40.95.\n",
      "Epoch   3: Train Loss: 28.963. Valid Loss: 25.449. Train Accuracy: -45.28. Valid Accuracy: -41.10.\n",
      "Epoch   4: Train Loss: 28.655. Valid Loss: 25.328. Train Accuracy: -44.94. Valid Accuracy: -40.84.\n",
      "Epoch   5: Train Loss: 28.463. Valid Loss: 25.830. Train Accuracy: -44.80. Valid Accuracy: -42.13.\n",
      "Epoch   6: Train Loss: 28.244. Valid Loss: 25.219. Train Accuracy: -44.63. Valid Accuracy: -40.60.\n",
      "Epoch   7: Train Loss: 28.169. Valid Loss: 25.349. Train Accuracy: -44.47. Valid Accuracy: -40.76.\n",
      "Epoch   8: Train Loss: 28.064. Valid Loss: 25.164. Train Accuracy: -44.49. Valid Accuracy: -40.32.\n",
      "Epoch   9: Train Loss: 27.895. Valid Loss: 25.001. Train Accuracy: -44.10. Valid Accuracy: -40.36.\n",
      "Epoch  10: Train Loss: 27.753. Valid Loss: 24.795. Train Accuracy: -44.21. Valid Accuracy: -40.31.\n",
      "Epoch  11: Train Loss: 27.670. Valid Loss: 25.523. Train Accuracy: -44.00. Valid Accuracy: -41.20.\n",
      "Epoch  12: Train Loss: 27.545. Valid Loss: 24.900. Train Accuracy: -44.05. Valid Accuracy: -40.24.\n",
      "Epoch  13: Train Loss: 27.561. Valid Loss: 24.826. Train Accuracy: -43.92. Valid Accuracy: -40.12.\n",
      "Epoch  14: Train Loss: 27.416. Valid Loss: 25.039. Train Accuracy: -43.66. Valid Accuracy: -40.51.\n",
      "Epoch  15: Train Loss: 27.427. Valid Loss: 24.941. Train Accuracy: -43.77. Valid Accuracy: -39.92.\n",
      "Epoch  16: Train Loss: 27.297. Valid Loss: 24.714. Train Accuracy: -43.59. Valid Accuracy: -40.16.\n",
      "Epoch  17: Train Loss: 27.179. Valid Loss: 24.696. Train Accuracy: -43.46. Valid Accuracy: -40.04.\n",
      "Epoch  18: Train Loss: 27.156. Valid Loss: 24.934. Train Accuracy: -43.41. Valid Accuracy: -40.86.\n",
      "Epoch  19: Train Loss: 27.102. Valid Loss: 24.687. Train Accuracy: -43.37. Valid Accuracy: -40.29.\n",
      "Epoch  20: Train Loss: 27.074. Valid Loss: 24.533. Train Accuracy: -43.40. Valid Accuracy: -39.89.\n",
      "Epoch  21: Train Loss: 26.980. Valid Loss: 24.648. Train Accuracy: -43.39. Valid Accuracy: -40.39.\n",
      "Epoch  22: Train Loss: 26.935. Valid Loss: 24.522. Train Accuracy: -43.20. Valid Accuracy: -39.70.\n",
      "Epoch  23: Train Loss: 26.892. Valid Loss: 24.824. Train Accuracy: -43.25. Valid Accuracy: -40.65.\n",
      "Epoch  24: Train Loss: 26.832. Valid Loss: 24.597. Train Accuracy: -43.15. Valid Accuracy: -40.01.\n",
      "Epoch  25: Train Loss: 26.766. Valid Loss: 24.512. Train Accuracy: -42.91. Valid Accuracy: -39.94.\n",
      "Epoch  26: Train Loss: 26.773. Valid Loss: 24.401. Train Accuracy: -43.06. Valid Accuracy: -39.72.\n",
      "Epoch  27: Train Loss: 26.700. Valid Loss: 24.345. Train Accuracy: -42.92. Valid Accuracy: -39.78.\n",
      "Epoch  28: Train Loss: 26.634. Valid Loss: 24.623. Train Accuracy: -42.94. Valid Accuracy: -40.01.\n",
      "Epoch  29: Train Loss: 26.573. Valid Loss: 24.334. Train Accuracy: -42.74. Valid Accuracy: -39.81.\n",
      "Epoch  30: Train Loss: 26.560. Valid Loss: 24.432. Train Accuracy: -42.90. Valid Accuracy: -39.71.\n"
     ]
    }
   ],
   "source": [
    "trainer(model, criterion, optimizer, train_loader, valid_loader, epochs=30, patience=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess testing data\n",
    "temp_averages= [\"january_avg_temp\",\"february_avg_temp\",\"march_avg_temp\",\"april_avg_temp\",\"may_avg_temp\",\n",
    "                \"june_avg_temp\",\"july_avg_temp\",\"august_avg_temp\",\"september_avg_temp\",\"october_avg_temp\",\"november_avg_temp\",\n",
    "                \"december_avg_temp\"]\n",
    "temp_mins= [\"january_min_temp\",\"february_min_temp\",\"march_min_temp\",\"april_min_temp\",\"may_min_temp\",\n",
    "                \"june_min_temp\",\"july_min_temp\",\"august_min_temp\",\"september_min_temp\",\"october_min_temp\",\"november_min_temp\",\n",
    "                \"december_min_temp\"]\n",
    "temp_max= [\"january_max_temp\",\"february_max_temp\",\"march_max_temp\",\"april_max_temp\",\"may_max_temp\",\n",
    "                \"june_max_temp\",\"july_max_temp\",\"august_max_temp\",\"september_max_temp\",\"october_max_temp\",\"november_max_temp\",\n",
    "                \"december_max_temp\"]\n",
    "\n",
    "df[\"months_above_65\"] =(df[temp_averages] >=65).sum(axis=1)\n",
    "df[\"months_below_65\"] =(df[temp_averages] <65).sum(axis=1)\n",
    "df[\"months_min_below_65\"] = (df[temp_mins] <65).sum(axis=1)\n",
    "df[\"months_min_above_65\"] = (df[temp_mins] >=65).sum(axis=1)\n",
    "df[\"months_max_below_65\"] = (df[temp_max] <65).sum(axis=1)\n",
    "df[\"months_max_above_65\"] = (df[temp_max] >=65).sum(axis=1)\n",
    "\n",
    "df = pd.merge(df, facility_class, on=\"facility_type\")\n",
    "\n",
    "id_submission = df[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = preprocessor.fit_transform(df)\n",
    "column_names_testing = (\n",
    "    numeric_features\n",
    "    + preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    "    .tolist()\n",
    ")\n",
    "X_test_transformed = pd.DataFrame(\n",
    "   X_test_transformed, columns=column_names_testing )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State factor 6 needs to be manually added in\n",
    "X_test_transformed['State_Factor_State_6'] = 0\n",
    "\n",
    "# reorder columns to be in same order as training set\n",
    "X_test_transformed = X_test_transformed[X_train_transformed_df.columns]\n",
    "\n",
    "# Change to tensor\n",
    "X_test_transformed= torch.tensor(X_test_transformed.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for i in range(0,X_test_transformed.shape[0]):\n",
    "    prediction.append( model(X_test_transformed[i]).item())\n",
    "    \n",
    "submission = pd.DataFrame({'id': id_submission, 'site_eui': prediction})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
