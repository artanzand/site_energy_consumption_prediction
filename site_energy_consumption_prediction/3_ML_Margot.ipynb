{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "# from lightgbm.sklearn import LGBMRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring metrics and CV score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = {\n",
    "    \"neg RMSE\": \"neg_root_mean_squared_error\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_data.csv')\n",
    "# X_test_submit = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any manual feature engineering before column transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_averages= [\"january_avg_temp\",\"february_avg_temp\",\"march_avg_temp\",\"april_avg_temp\",\"may_avg_temp\",\n",
    "                \"june_avg_temp\",\"july_avg_temp\",\"august_avg_temp\",\"september_avg_temp\",\"october_avg_temp\",\"november_avg_temp\",\n",
    "                \"december_avg_temp\"]\n",
    "temp_mins= [\"january_min_temp\",\"february_min_temp\",\"march_min_temp\",\"april_min_temp\",\"may_min_temp\",\n",
    "                \"june_min_temp\",\"july_min_temp\",\"august_min_temp\",\"september_min_temp\",\"october_min_temp\",\"november_min_temp\",\n",
    "                \"december_min_temp\"]\n",
    "temp_max= [\"january_max_temp\",\"february_max_temp\",\"march_max_temp\",\"april_max_temp\",\"may_max_temp\",\n",
    "                \"june_max_temp\",\"july_max_temp\",\"august_max_temp\",\"september_max_temp\",\"october_max_temp\",\"november_max_temp\",\n",
    "                \"december_max_temp\"]\n",
    "\n",
    "df[\"months_above_65\"] =(df[temp_averages] >=65).sum(axis=1)\n",
    "df[\"months_below_65\"] =(df[temp_averages] <65).sum(axis=1)\n",
    "df[\"months_min_below_65\"] = (df[temp_mins] <65).sum(axis=1)\n",
    "df[\"months_min_above_65\"] = (df[temp_mins] >=65).sum(axis=1)\n",
    "df[\"months_max_below_65\"] = (df[temp_max] <65).sum(axis=1)\n",
    "df[\"months_max_above_65\"] = (df[temp_max] >=65).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Grouping \n",
    "\n",
    "df.loc[df.facility_type.isin([\"Commercial_Unknown\",\n",
    "                            \"Commercial_Other\",\n",
    "                            \"Mixed_Use_Predominantly_Commercial\",\n",
    "                            \"Mixed_Use_Commercial_and_Residential\"]), 'b_subtype'] = \"commercial_other\"\n",
    "\n",
    "df.loc[df.facility_type.isin([\"Data_Center\", \n",
    "                                \"Laboratory\"]), 'b_subtype'] = \"science\"\n",
    "                                    \n",
    "df.loc[df.facility_type.isin([\"Education_Other_classroom\",\n",
    "                \"Education_College_or_university\"\n",
    "                \"Education_Uncategorized\"\n",
    "                \"Education_Preschool_or_daycare\"]), 'b_subtype'] = \"education\"   \n",
    "\n",
    "\n",
    "df.loc[df.facility_type.isin([\"Public_Assembly_Entertainment_culture\",                              \n",
    "                \"Public_Assembly_Drama_theater\",\n",
    "                \"Public_Assembly_Social_meeting\",\n",
    "                \"Public_Assembly_Recreation\",\n",
    "                \"Public_Assembly_Movie_Theater\",\n",
    "                \"Public_Assembly_Library\",\n",
    "                \"Public_Safety_Uncategorized\",\n",
    "                \"Public_Safety_Fire_or_police_station\",\n",
    "                \"Public_Assembly_Other\",\n",
    "                \"Public_Safety_Penitentiary\",\n",
    "                \"Public_Safety_Courthouse\",\n",
    "                \"Public_Assembly_Stadium\",\n",
    "                \"Public_Assembly_Uncategorized\",\n",
    "                \"Religious_worship\",\n",
    "                \"Parking_Garage\"]), 'b_subtype'] = \"public\"   \n",
    "\n",
    "df.loc[df.facility_type.isin([\"Warehouse_Distribution_or_Shipping_center\",\n",
    "                \"Warehouse_Nonrefrigerated\",\n",
    "                \"Warehouse_Selfstorage\",\n",
    "                \"Warehouse_Uncategorized\",\n",
    "                \"Warehouse_Refrigerated\"]), 'b_subtype'] = \"warehouse\"  \n",
    "\n",
    "df.loc[df.facility_type.isin([\"Retail_Vehicle_dealership_showroom\",\n",
    "            \"Retail_Uncategorized\",\n",
    "            \"Retail_Strip_shopping_mall\",\n",
    "            \"Retail_Enclosed_mall\"]), 'b_subtype'] = \"retail\"  \n",
    "\n",
    "\n",
    "df.loc[df.facility_type.isin([\"Food_Service_Uncategorized\",\n",
    "            \"Food_Service_Other\",\n",
    "            \"Food_Service_Restaurant_or_cafeteria\",\n",
    "            \"Food_Sales\",\n",
    "            \"Grocery_store_or_food_market\"]), 'b_subtype'] = \"food\"  \n",
    "\n",
    "df.loc[df.facility_type.isin([\"Nursing_Home\",\n",
    "            \"Lodging_Dormitory_or_fraternity_sorority\",\n",
    "            \"Lodging_Other\",\n",
    "            \"Lodging_Uncategorized\",\n",
    "            \"Lodging_Hotel\",\n",
    "            \"Mixed_Use_Predominantly_Residential\"]), 'b_subtype'] = \"lodging\" \n",
    "\n",
    "df.loc[df.facility_type.isin([\"Office_Bank_or_other_financial\"\n",
    "            \"Office_Mixed_use\",\n",
    "            \"Office_Uncategorized\",\n",
    "            \"Office_Medical_non_diagnostic\"]), 'b_subtype'] = \"office\" \n",
    "\n",
    "df.loc[df.facility_type.isin([\"Service_Vehicle_service_repair_shop\",\n",
    "            \"Service_Drycleaning_or_Laundry\",\n",
    "            \"Service_Uncategorized\"]), 'b_subtype'] = \"survice\" \n",
    "\n",
    "df.loc[df.facility_type.isin([\"Health_Care_Outpatient_Uncategorized\",\n",
    "            \"Health_Care_Inpatient\",\n",
    "            \"Health_Care_Uncategorized\",\n",
    "            \"Health_Care_Outpatient_Clinic\"]), 'b_subtype'] = \"health_care\" \n",
    "\n",
    "\n",
    "df.loc[df.facility_type == \"Industrial\", 'b_subtype'] = \"industrial\" \n",
    "\n",
    "df.loc[df.facility_type == \"2to4_Unit_Building\", 'b_subtype'] = \"2to4_unit\" \n",
    "\n",
    "df.loc[df.facility_type == \"5plus_Unit_Building\", 'b_subtype'] = \"5_unit\" \n",
    "\n",
    "df.loc[df.facility_type == \"Multifamily_Uncategorized\", 'b_subtype'] = \"multifamily\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group columns for transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"site_eui\"\n",
    "\n",
    "numeric_features = [\n",
    "    \"Year_Factor\",\n",
    "    \"floor_area\",\n",
    "    \"year_built\",\n",
    "    \"energy_star_rating\", # Nan to 0?\n",
    "    \"ELEVATION\",\n",
    "    \"january_avg_temp\",\n",
    "    \"february_avg_temp\",\n",
    "    \"march_avg_temp\",\n",
    "    \"april_avg_temp\",\n",
    "    \"may_avg_temp\",\n",
    "    \"june_avg_temp\",\n",
    "    \"july_avg_temp\",\n",
    "    \"august_avg_temp\",\n",
    "    \"september_avg_temp\",\n",
    "    \"october_avg_temp\",\n",
    "    \"november_avg_temp\",\n",
    "    \"december_avg_temp\",\n",
    "    \"cooling_degree_days\",\n",
    "    \"heating_degree_days\",\n",
    "    \"precipitation_inches\",\n",
    "    \"snowdepth_inches\",\n",
    "    \"avg_temp\",\n",
    "    \"days_below_30F\",\n",
    "    \"days_below_20F\",\n",
    "    \"days_below_10F\",\n",
    "    \"days_below_0F\",\n",
    "    \"days_above_80F\",\n",
    "    \"days_above_90F\",\n",
    "    \"days_above_100F\",\n",
    "    \"days_above_110F\",\n",
    "    \"max_wind_speed\",\n",
    "    \"months_above_65\",\n",
    "    \"months_below_65\"\n",
    "]\n",
    "\n",
    "ordinal_features = []\n",
    "categorical_features = [\"State_Factor\",\n",
    "                        \"building_class\",\n",
    "                        'b_subtype']\n",
    "\n",
    "drop_features = [\n",
    "    \"id\",\n",
    "    \"january_min_temp\",\n",
    "    \"january_max_temp\",\n",
    "    \"february_max_temp\",\n",
    "    \"february_min_temp\",\n",
    "    \"march_min_temp\",\n",
    "    \"march_max_temp\",\n",
    "    \"april_min_temp\",\n",
    "    \"april_max_temp\",\n",
    "    \"may_min_temp\",\n",
    "    \"may_max_temp\",\n",
    "    \"june_min_temp\",\n",
    "    \"june_max_temp\",\n",
    "    \"july_min_temp\",\n",
    "    \"july_max_temp\",\n",
    "    \"august_min_temp\",\n",
    "    \"august_max_temp\",\n",
    "    \"september_min_temp\",\n",
    "    \"september_max_temp\",\n",
    "    \"october_min_temp\",\n",
    "    \"october_max_temp\",\n",
    "    \"november_min_temp\",\n",
    "    \"november_max_temp\",\n",
    "    \"december_min_temp\",\n",
    "    \"december_max_temp\",\n",
    "    \"snowfall_inches\",\n",
    "    \"direction_peak_wind_speed\",\n",
    "    \"direction_max_wind_speed\",\n",
    "    \"days_with_fog\",\n",
    "    \"facility_type\",\n",
    "    \"months_min_below_65\",\n",
    "    \"months_min_above_65\",\n",
    "    \"months_max_below_65\",\n",
    "    \"months_max_above_65\"]\n",
    "\n",
    "assert df.columns.shape[0] == len(\n",
    "    numeric_features\n",
    "    + ordinal_features\n",
    "    + categorical_features\n",
    "    + [target]\n",
    "    + drop_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=123)\n",
    "X_train, y_train = train_df.drop(columns=[target]), train_df[target]\n",
    "X_test, y_test = test_df.drop(columns=[target]), test_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column transformation & preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"mean\"), StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check transformed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Factor</th>\n",
       "      <th>floor_area</th>\n",
       "      <th>year_built</th>\n",
       "      <th>energy_star_rating</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>january_avg_temp</th>\n",
       "      <th>february_avg_temp</th>\n",
       "      <th>march_avg_temp</th>\n",
       "      <th>april_avg_temp</th>\n",
       "      <th>may_avg_temp</th>\n",
       "      <th>...</th>\n",
       "      <th>b_subtype_industrial</th>\n",
       "      <th>b_subtype_lodging</th>\n",
       "      <th>b_subtype_multifamily</th>\n",
       "      <th>b_subtype_office</th>\n",
       "      <th>b_subtype_public</th>\n",
       "      <th>b_subtype_retail</th>\n",
       "      <th>b_subtype_science</th>\n",
       "      <th>b_subtype_survice</th>\n",
       "      <th>b_subtype_warehouse</th>\n",
       "      <th>b_subtype_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69130</th>\n",
       "      <td>1.107399</td>\n",
       "      <td>2.675386</td>\n",
       "      <td>-0.689701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.351574</td>\n",
       "      <td>-0.642124</td>\n",
       "      <td>-0.185602</td>\n",
       "      <td>0.224124</td>\n",
       "      <td>-1.542441</td>\n",
       "      <td>-1.638004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27033</th>\n",
       "      <td>-0.930308</td>\n",
       "      <td>-0.410844</td>\n",
       "      <td>-0.689701</td>\n",
       "      <td>-1.263114</td>\n",
       "      <td>-0.229869</td>\n",
       "      <td>0.109424</td>\n",
       "      <td>-0.184002</td>\n",
       "      <td>-0.651749</td>\n",
       "      <td>-0.320761</td>\n",
       "      <td>-0.338626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>-0.251072</td>\n",
       "      <td>-0.501988</td>\n",
       "      <td>0.073207</td>\n",
       "      <td>0.124062</td>\n",
       "      <td>1.350624</td>\n",
       "      <td>2.973102</td>\n",
       "      <td>2.183968</td>\n",
       "      <td>2.189378</td>\n",
       "      <td>2.527678</td>\n",
       "      <td>-0.090301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11958</th>\n",
       "      <td>0.428163</td>\n",
       "      <td>-0.433457</td>\n",
       "      <td>-1.588842</td>\n",
       "      <td>1.121094</td>\n",
       "      <td>2.707715</td>\n",
       "      <td>-1.771235</td>\n",
       "      <td>-2.241353</td>\n",
       "      <td>-1.551884</td>\n",
       "      <td>-2.235285</td>\n",
       "      <td>-1.251078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.288779</td>\n",
       "      <td>0.433997</td>\n",
       "      <td>0.073207</td>\n",
       "      <td>-0.699574</td>\n",
       "      <td>-0.626660</td>\n",
       "      <td>2.315448</td>\n",
       "      <td>1.699886</td>\n",
       "      <td>1.386293</td>\n",
       "      <td>0.686532</td>\n",
       "      <td>-2.463830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year_Factor  floor_area  year_built  energy_star_rating  ELEVATION  \\\n",
       "69130     1.107399    2.675386   -0.689701            0.000000  -0.351574   \n",
       "27033    -0.930308   -0.410844   -0.689701           -1.263114  -0.229869   \n",
       "2610     -0.251072   -0.501988    0.073207            0.124062   1.350624   \n",
       "11958     0.428163   -0.433457   -1.588842            1.121094   2.707715   \n",
       "1        -2.288779    0.433997    0.073207           -0.699574  -0.626660   \n",
       "\n",
       "       january_avg_temp  february_avg_temp  march_avg_temp  april_avg_temp  \\\n",
       "69130         -0.642124          -0.185602        0.224124       -1.542441   \n",
       "27033          0.109424          -0.184002       -0.651749       -0.320761   \n",
       "2610           2.973102           2.183968        2.189378        2.527678   \n",
       "11958         -1.771235          -2.241353       -1.551884       -2.235285   \n",
       "1              2.315448           1.699886        1.386293        0.686532   \n",
       "\n",
       "       may_avg_temp  ...  b_subtype_industrial  b_subtype_lodging  \\\n",
       "69130     -1.638004  ...                   0.0                0.0   \n",
       "27033     -0.338626  ...                   0.0                1.0   \n",
       "2610      -0.090301  ...                   0.0                0.0   \n",
       "11958     -1.251078  ...                   0.0                0.0   \n",
       "1         -2.463830  ...                   0.0                0.0   \n",
       "\n",
       "       b_subtype_multifamily  b_subtype_office  b_subtype_public  \\\n",
       "69130                    0.0               0.0               0.0   \n",
       "27033                    0.0               0.0               0.0   \n",
       "2610                     0.0               0.0               0.0   \n",
       "11958                    0.0               0.0               0.0   \n",
       "1                        0.0               0.0               0.0   \n",
       "\n",
       "       b_subtype_retail  b_subtype_science  b_subtype_survice  \\\n",
       "69130               0.0                0.0                0.0   \n",
       "27033               0.0                0.0                0.0   \n",
       "2610                1.0                0.0                0.0   \n",
       "11958               0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "\n",
       "       b_subtype_warehouse  b_subtype_nan  \n",
       "69130                  0.0            0.0  \n",
       "27033                  0.0            0.0  \n",
       "2610                   0.0            0.0  \n",
       "11958                  0.0            0.0  \n",
       "1                      1.0            0.0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = (\n",
    "    numeric_features\n",
    "    + preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(\n",
    "    X_train_transformed, columns=column_names, index=X_train.index\n",
    ")\n",
    "\n",
    "X_train_transformed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy regressor as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg RMSE</th>\n",
       "      <th>train_neg RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.014 (+/- 0.005)</td>\n",
       "      <td>0.001 (+/- 0.002)</td>\n",
       "      <td>-58.543 (+/- 1.843)</td>\n",
       "      <td>-58.563 (+/- 0.456)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fit_time         score_time        test_neg RMSE  \\\n",
       "Dummy  0.014 (+/- 0.005)  0.001 (+/- 0.002)  -58.543 (+/- 1.843)   \n",
       "\n",
       "            train_neg RMSE  \n",
       "Dummy  -58.563 (+/- 0.456)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "pipe_dummy = DummyRegressor()\n",
    "results[\"Dummy\"] = mean_std_cross_val_scores(\n",
    "    pipe_dummy, X_train, y_train, return_train_score=True, scoring=scoring_metrics\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train several models (CV) and retrieve the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ridge = make_pipeline(preprocessor, Ridge(random_state=123))\n",
    "\n",
    "pipe_rf = make_pipeline(\n",
    "    preprocessor, RandomForestRegressor(random_state=123, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# pipe_xgb = make_pipeline(\n",
    "#     preprocessor, XGBRegressor(random_state=123, n_jobs=-1, verbosity=0)\n",
    "# )\n",
    "\n",
    "# pipe_lgbm = make_pipeline(preprocessor, LGBMRegressor(random_state=123))\n",
    "\n",
    "pipe_catboost = make_pipeline(\n",
    "    preprocessor, CatBoostRegressor(random_state=123, verbose=0)\n",
    ")\n",
    "\n",
    "models = {\n",
    "    # \"Ridge\": pipe_ridge,\n",
    "    # \"Random Forest\": pipe_rf,\n",
    "    # \"XGBoost\": pipe_xgb,\n",
    "    # \"LightGBM\": pipe_lgbm,\n",
    "    \"CatBoost\": pipe_catboost,\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    results[model_name] = mean_std_cross_val_scores(\n",
    "        model, X_train, y_train, return_train_score=True, scoring=scoring_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg RMSE</th>\n",
       "      <th>train_neg RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.014 (+/- 0.005)</td>\n",
       "      <td>0.001 (+/- 0.002)</td>\n",
       "      <td>-58.543 (+/- 1.843)</td>\n",
       "      <td>-58.563 (+/- 0.456)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>7.788 (+/- 1.172)</td>\n",
       "      <td>0.266 (+/- 0.019)</td>\n",
       "      <td>-41.998 (+/- 1.598)</td>\n",
       "      <td>-35.311 (+/- 0.359)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fit_time         score_time        test_neg RMSE  \\\n",
       "Dummy     0.014 (+/- 0.005)  0.001 (+/- 0.002)  -58.543 (+/- 1.843)   \n",
       "CatBoost  7.788 (+/- 1.172)  0.266 (+/- 0.019)  -41.998 (+/- 1.598)   \n",
       "\n",
       "               train_neg RMSE  \n",
       "Dummy     -58.563 (+/- 0.456)  \n",
       "CatBoost  -35.311 (+/- 0.359)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Year_Factor', 'floor_area',\n",
       "                                                   'year_built',\n",
       "                                                   'energy_star_rating',\n",
       "                                                   'ELEVATION',\n",
       "                                                   'january_avg_temp',\n",
       "                                                   'february_avg_temp',\n",
       "                                                   'march_avg_temp',\n",
       "                                                   'april_avg_temp',\n",
       "                                                   'may_avg_temp',\n",
       "                                                   'june_avg_temp',\n",
       "                                                   '...\n",
       "                                                   'september_min_temp',\n",
       "                                                   'september_max_temp',\n",
       "                                                   'october_min_temp',\n",
       "                                                   'october_max_temp',\n",
       "                                                   'november_min_temp',\n",
       "                                                   'november_max_temp',\n",
       "                                                   'december_min_temp',\n",
       "                                                   'december_max_temp',\n",
       "                                                   'snowfall_inches',\n",
       "                                                   'direction_peak_wind_speed',\n",
       "                                                   'direction_max_wind_speed',\n",
       "                                                   'days_with_fog',\n",
       "                                                   'facility_type', ...])])),\n",
       "                ('catboostregressor',\n",
       "                 <catboost.core.CatBoostRegressor object at 0x00000152A1B74CA0>)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets, utils\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class energy_model(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 30),\n",
    "            nn.LeakyReLU(),\n",
    "            torch.nn.Linear(30, 20),\n",
    "            nn.LeakyReLU(),\n",
    "            torch.nn.Linear(20, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            torch.nn.Linear(5, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, trainloader, validloader, epochs=5, patience=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    def RMSELoss(yhat,y):\n",
    "        return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    \n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        \n",
    "        train_batch_loss = 0\n",
    "        train_batch_acc = 0\n",
    "        valid_batch_loss = 0\n",
    "        valid_batch_acc = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            y_hat = model(X.view(X.shape[0], -1)).flatten()\n",
    "            loss = RMSE(yhat,y)   # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "            train_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()   # Average accuracy for this batch\n",
    "            \n",
    "        train_loss.append(train_batch_loss / len(trainloader))     # loss = total loss in epoch / number of batches = loss per batch\n",
    "        train_accuracy.append(train_batch_acc / len(trainloader))  # accuracy\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()  # this turns off those random dropout layers, we don't want them for validation!\n",
    "        \n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "            for X, y in validloader:\n",
    "                y_hat = model(X.view(X.shape[0], -1)).flatten()  # Forward pass to get output\n",
    "                y_hat_labels = torch.sigmoid(y_hat) > 0.5        # convert probabilities to False (0) and True (1)\n",
    "                loss = RMSE(yhat,y)   # Calculate loss based on output\n",
    "                valid_batch_loss += loss.item()                  # Add loss for this batch to running total\n",
    "                valid_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()   # Average accuracy for this batch\n",
    "                \n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n",
    "        \n",
    "        model.train()  # turn back on the dropout layers for the next training loop\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1:3}:\",\n",
    "                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n",
    "                  f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n",
    "                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 0 and valid_loss[-1] > valid_loss[-2]:\n",
    "            consec_increases += 1\n",
    "        else:\n",
    "            consec_increases = 0\n",
    "        if consec_increases == patience:\n",
    "            print(f\"Stopped early at epoch {epoch + 1:3}: val loss increased for {consec_increases} consecutive epochs!\")\n",
    "            break\n",
    "    \n",
    "    results = {\"train_loss\": train_loss,\n",
    "               \"valid_loss\": valid_loss,\n",
    "               \"train_accuracy\": train_accuracy,\n",
    "               \"valid_accuracy\": valid_accuracy}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1 is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m testing_data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_data.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(testing_data, batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(model, optimizer, trainloader, validloader, epochs, patience, verbose)\u001b[0m\n\u001b[0;32m     17\u001b[0m valid_batch_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()       \u001b[38;5;66;03m# Zero all the gradients w.r.t. parameters\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m model(X\u001b[38;5;241m.\u001b[39mview(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\site_energy_consumption_prediction\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\site_energy_consumption_prediction\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\site_energy_consumption_prediction\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\site_energy_consumption_prediction\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\site_energy_consumption_prediction\\lib\\site-packages\\numpy\\lib\\npyio.py:249\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a file in the archive\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '1 is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "input_size = len(X_train_transformed_df.columns)-1\n",
    "model = energy_model(input_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "np.savez('train_data',input=X_train,targets=y_train)\n",
    "training_data=np.load('train_data.npz')\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size= 200, shuffle=True)\n",
    "\n",
    "np.savez('test_data',input=X_test,targets=y_test)\n",
    "testing_data=np.load('test_data.npz')\n",
    "valid_loader = torch.utils.data.DataLoader(testing_data, batch_size= 200, shuffle=True)\n",
    "\n",
    "\n",
    "trainer(model, optimizer, train_loader, valid_loader, epochs=3, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "     \"catboost_depth\":np.arange(2, 20, 1),\n",
    "     \"catboost_iterations\":np.arange(2, 20, 1)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_catboost,\n",
    "    param_distributions=param_grid,\n",
    "    n_jobs=-1,\n",
    "    n_iter=5,\n",
    "    cv=5,\n",
    "    random_state=123,\n",
    ")\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipe_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_fitted = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4981037009181184"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score = pipe.score(X_test, y_test)\n",
    "final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>site_eui</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75757</td>\n",
       "      <td>283.555634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75758</td>\n",
       "      <td>230.533096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75759</td>\n",
       "      <td>341.535339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75760</td>\n",
       "      <td>281.239624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75761</td>\n",
       "      <td>297.303864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    site_eui\n",
       "0  75757  283.555634\n",
       "1  75758  230.533096\n",
       "2  75759  341.535339\n",
       "3  75760  281.239624\n",
       "4  75761  297.303864"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': X_test_submit[\"id\"], 'site_eui': pipe_xgb.predict(X_test_submit)})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
